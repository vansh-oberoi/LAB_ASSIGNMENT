{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tSn7I_k3FQ1",
    "outputId": "ae8c7e96-670c-448d-fa74-c474beecde1e"
   },
   "outputs": [],
   "source": [
    "#Installation Cell\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q opencv-python matplotlib numpy Pillow\n",
    "!pip install -q 'git+https://github.com/facebookresearch/segment-anything-2.git'\n",
    "!pip install -q transformers\n",
    "!pip install -q supervision\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import supervision as sv\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3COro_p7_er",
    "outputId": "74de5b44-a678-439a-f129-9b96e0f63d42"
   },
   "outputs": [],
   "source": [
    "#SAM 2 Setup\n",
    "import torch\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "# Download SAM 2 model checkpoint\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt\n",
    "\n",
    "# Initialize SAM 2 model\n",
    "model_config = \"sam2_hiera_l.yaml\"\n",
    "checkpoint = \"sam2_hiera_large.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam2_model = build_sam2(model_config, checkpoint, device=device)\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "\n",
    "print(f\"SAM 2 model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386,
     "referenced_widgets": [
      "18e174f1cece4443bbfccbfecff3e254",
      "a02c8c1f4f9b4d28b2edeeab33495f04",
      "e07cfaa2b5024ac1a8b38fa046200669",
      "7a8b029ed038492f8a662e55a7a65c97",
      "2ab49777256c4831a440cfdacbf8acef",
      "1555081d8c9e4f93b39fa1c36f50a1c4",
      "848bf606278f4ecd8b23d2bae8696215",
      "d77d025ee5a8427688d5ca67e8554e75",
      "902cbc26a88945ee97a1f409ea19ea65",
      "5eddfa8bf1914ae9b989632d2039f8dd",
      "9edb605d4b414eeda2d0584b47c9f9a4",
      "e10f5ac9ab504700a6bde2a412bbab18",
      "8d571464b6ff48ffb5b92d0afd4a2db5",
      "efcc16cba70544d587eb085c837d1807",
      "1a627f91097b4dc5b311647ee18a2186",
      "ab3681d8763f40fe8a646bb5167a17cf",
      "3d3e67ec9e244e89950277540f587837",
      "d58460e87e764bc9a059f584dbdc2be3",
      "20a6ed27677b44708c05f0a7cc0f92bd",
      "caf70a10089b43c0b27833338d75b4b8",
      "47cb26618d2940edb727b44894eb4642",
      "dcdb8d26ba5f4513b9aff499625ce1cf",
      "38f4e3dc63d84985a551220aa898cef2",
      "eb6e2859c467421bba1464a81f3c4672",
      "708dee98498f448886f468d9ce869a0f",
      "249152d9b4e1448bb8fb283b2a8ead1c",
      "f282c4fbf7254967b450f6065bdb84ed",
      "e973db5693aa469386e48472ddfb57e5",
      "4a857436580247a0a62511d0543db75e",
      "af25998a60d34f5e96941f6660bc0887",
      "4e6ba5989b594b3a9a566c26a6247624",
      "ce9c9175a2b84f7397b19d15c7c9688f",
      "df30eaa988fb4b899a79481690d97b0d",
      "76fc9bc57b4943a8873b7703aded8a90",
      "42d5bf2cbfb941259b8f81b7025e333e",
      "52730e672c634dc39c0deeda8486a6d6",
      "0cbb349f154b4399803ebe67287ecc8d",
      "fc6554f92cad4deeb293d2b1efab29c4",
      "887afa6471d24f81ad40d098bfe58869",
      "e1e0422152454fea9b272eabb5cb6492",
      "66248c20b4584abc8672e6ab1022e1b4",
      "f3ce103b71c84797b8d9c68938691555",
      "fbfbcb8d04704f6285cd06ce9e15dc2c",
      "fb2f6b5c6c444f7da481c5d2f621ae9a",
      "98e04f32ff404d0aa94c674020c831e8",
      "3bff2e899a7f4636b6c58864e3c9ff0d",
      "fd8bd435b7374b3899ff693a92f25c04",
      "dc277bb04aaf45a697390f3756c5e02c",
      "cadd629d1e594a2db1d737bdb6c12edf",
      "53e7bd20b0fd4f3fa6e01491b2220fb3",
      "0478962f33ec42fe8352eabd36661901",
      "4da39b5411e74094bb608ebe456f5e0c",
      "275a09d0008347449e365c384f07398f",
      "f65ec846020842738651152634dfe870",
      "5215acab9afd49ba857548ea3aee9fc8",
      "7e37378fbef64a8da947a865a033c5d5",
      "3701579916c84dcab1a3cfd3ee1984f8",
      "e57e6dfed54b42279f2214fc6e943813",
      "7febcad86fa84bd781ba5a67759d034e",
      "ebb676415005409ca7cd07b97b37de7a",
      "0a7864173c784262979e8a43c4e56ff4",
      "ec12794d43924980bf9a5a50b25d4598",
      "492c91b8ddea42fe837444e8b0137600",
      "62688e22831147d8a3a8b17c67142284",
      "76b9b125d3b04db0b0b71fef066b0041",
      "bdc015a1dee14e23b13919e4a56c0b88",
      "c4f112610f2f484cb46b2f926a5384bb",
      "f0f4043574394d19a4fa7a178abeac92",
      "34e2f3492b254701a404d4f4ba5b1451",
      "d5697c5615ab4601b174d1355ca25d38",
      "06496bd48ab1454f850f225edaa685ce",
      "701767e368f049328b362ef10aa45874",
      "279a6bbab403401494276cd4bb0e7c33",
      "4df2d47ae2dc4a0eb8d3a3176655c780",
      "8d1c587964e0476385196019d87b1de5",
      "6b34fa7d5d3c4d618001e26335ed85be",
      "9182977e55674e06bc056f089faf8166"
     ]
    },
    "id": "cZpnY81K8BwP",
    "outputId": "355b308f-dd16-4416-d302-d87cc862366c"
   },
   "outputs": [],
   "source": [
    "#Text-to-Mask Pipeline Setup\n",
    "class TextToMaskPipeline:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        # Use a zero-shot object detection model\n",
    "        self.detector = pipeline(\n",
    "            \"zero-shot-object-detection\",\n",
    "            model=\"google/owlvit-base-patch32\",\n",
    "            device=0 if device == \"cuda\" else -1\n",
    "        )\n",
    "\n",
    "    def text_to_bounding_box(self, image, text_prompt, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Convert text prompt to bounding boxes using zero-shot detection\n",
    "        \"\"\"\n",
    "        # Convert PIL image to RGB if needed\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            image_pil = image.convert('RGB')\n",
    "\n",
    "        # Run detection\n",
    "        detections = self.detector(image_pil, candidate_labels=[text_prompt])\n",
    "\n",
    "        # Filter by confidence threshold\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        for detection in detections:\n",
    "            if detection['score'] >= threshold:\n",
    "                box = detection['box']\n",
    "                # Convert to [x1, y1, x2, y2] format\n",
    "                x1, y1, width, height = box['xmin'], box['ymin'], box['xmax'] - box['xmin'], box['ymax'] - box['ymin']\n",
    "                boxes.append([x1, y1, x1 + width, y1 + height])\n",
    "                scores.append(detection['score'])\n",
    "\n",
    "        return np.array(boxes), np.array(scores)\n",
    "\n",
    "# Initialize the pipeline\n",
    "text_pipeline = TextToMaskPipeline(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Igt-W5Q8GWR"
   },
   "outputs": [],
   "source": [
    "#Utility Functions\n",
    "def load_image_from_url(url):\n",
    "    \"\"\"Load image from URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return np.array(image)\n",
    "\n",
    "def load_image_from_path(path):\n",
    "    \"\"\"Load image from local path\"\"\"\n",
    "    image = Image.open(path)\n",
    "    return np.array(image)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*',\n",
    "               s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*',\n",
    "               s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q54Hdkqq8MyQ"
   },
   "outputs": [],
   "source": [
    "# @title Main Segmentation Function (Fixed)\n",
    "def text_driven_segmentation(image, text_prompt, confidence_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Perform text-driven segmentation on an image\n",
    "\n",
    "    Args:\n",
    "        image: numpy array or PIL Image\n",
    "        text_prompt: string describing the object to segment\n",
    "        confidence_threshold: confidence threshold for object detection\n",
    "\n",
    "    Returns:\n",
    "        masks: list of segmentation masks\n",
    "        boxes: list of bounding boxes\n",
    "        image_with_overlay: image with mask overlay\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to numpy array if needed\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_np = np.array(image)\n",
    "    else:\n",
    "        image_np = image.copy()\n",
    "\n",
    "    # Set image for predictor\n",
    "    predictor.set_image(image_np)\n",
    "\n",
    "    # Get bounding boxes from text prompt\n",
    "    boxes, scores = text_pipeline.text_to_bounding_box(\n",
    "        image_np, text_prompt, threshold=confidence_threshold\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(boxes)} detection(s) for '{text_prompt}'\")\n",
    "\n",
    "    all_masks = []\n",
    "    all_boxes = []\n",
    "\n",
    "    # Process each detected box\n",
    "    for i, box in enumerate(boxes):\n",
    "        print(f\"Processing detection {i+1} with confidence {scores[i]:.3f}\")\n",
    "\n",
    "        # Convert box to SAM 2 format\n",
    "        input_box = np.array(box)\n",
    "\n",
    "        # Predict mask\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            box=input_box[None, :],\n",
    "            multimask_output=True,\n",
    "        )\n",
    "\n",
    "        # Select the best mask\n",
    "        best_mask_idx = np.argmax(scores)\n",
    "        best_mask = masks[best_mask_idx]\n",
    "\n",
    "        all_masks.append(best_mask)\n",
    "        all_boxes.append(box)\n",
    "\n",
    "    # Create overlay visualization - FIXED VERSION\n",
    "    image_with_overlay = image_np.copy()\n",
    "\n",
    "    for mask in all_masks:\n",
    "        # Ensure mask is boolean\n",
    "        if mask.dtype != bool:\n",
    "            mask = mask.astype(bool)\n",
    "\n",
    "        color = np.array([30, 144, 255])  # Blue color\n",
    "\n",
    "        # Apply mask overlay correctly\n",
    "        for c in range(3):  # For each color channel\n",
    "            image_with_overlay[mask, c] = (\n",
    "                image_with_overlay[mask, c] * 0.5 + color[c] * 0.5\n",
    "            )\n",
    "\n",
    "    return all_masks, all_boxes, image_with_overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "HnoAL9dK8RBK",
    "outputId": "64d4c93d-b6b2-408d-e43b-898cc103bbb0"
   },
   "outputs": [],
   "source": [
    "# @title Demo with Sample Image\n",
    "# @markdown Run this cell to test the pipeline with a sample image\n",
    "\n",
    "# Load a sample image\n",
    "sample_url = \"https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\"\n",
    "print(\"Loading sample image...\")\n",
    "image = load_image_from_url(sample_url)\n",
    "\n",
    "# Text prompt\n",
    "text_prompt = \"dog\"  # @param {type:\"string\"}\n",
    "confidence_threshold = 0.1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "\n",
    "print(f\"Processing image with text prompt: '{text_prompt}'\")\n",
    "\n",
    "# Perform segmentation\n",
    "masks, boxes, result_image = text_driven_segmentation(\n",
    "    image, text_prompt, confidence_threshold\n",
    ")\n",
    "\n",
    "# Display results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Image with bounding boxes\n",
    "axes[1].imshow(image)\n",
    "for box in boxes:\n",
    "    show_box(box, axes[1])\n",
    "axes[1].set_title('Detected Bounding Boxes')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Image with mask overlay\n",
    "axes[2].imshow(result_image)\n",
    "axes[2].set_title('Segmentation Mask Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Segmentation completed! Found {len(masks)} object(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 56
    },
    "id": "GQEBuv5E8TIz",
    "outputId": "df0a6a19-6743-4521-b4ab-777d0219b73b"
   },
   "outputs": [],
   "source": [
    "# @title Custom Image Upload and Processing\n",
    "# @markdown Upload your own image and specify a text prompt\n",
    "\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "# Upload image\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    # Get the first uploaded file\n",
    "    file_name = list(uploaded.keys())[0]\n",
    "    image = load_image_from_path(file_name)\n",
    "\n",
    "    # Text prompt input\n",
    "    custom_prompt = \"person\"  # @param {type:\"string\"}\n",
    "    custom_confidence = 0.1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "\n",
    "    print(f\"Processing uploaded image with text prompt: '{custom_prompt}'\")\n",
    "\n",
    "    # Perform segmentation\n",
    "    masks, boxes, result_image = text_driven_segmentation(\n",
    "        image, custom_prompt, custom_confidence\n",
    "    )\n",
    "\n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Image with bounding boxes\n",
    "    axes[1].imshow(image)\n",
    "    for box in boxes:\n",
    "        show_box(box, axes[1])\n",
    "    axes[1].set_title('Detected Bounding Boxes')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Image with mask overlay\n",
    "    axes[2].imshow(result_image)\n",
    "    axes[2].set_title('Segmentation Mask Overlay')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Segmentation completed! Found {len(masks)} object(s).\")\n",
    "else:\n",
    "    print(\"No image uploaded. Please run the cell again to upload an image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 889
    },
    "id": "6_Ix2Y408W4m",
    "outputId": "d047040c-2462-4dca-b60d-2e308a523a1d"
   },
   "outputs": [],
   "source": [
    "# @title Fixed: Multiple Object Detection\n",
    "# @markdown Try detecting multiple objects with different text prompts\n",
    "\n",
    "def multi_object_segmentation_fixed(image, text_prompts, confidence_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Segment multiple objects specified by different text prompts - FIXED VERSION\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_np = np.array(image)\n",
    "    else:\n",
    "        image_np = image.copy()\n",
    "\n",
    "    predictor.set_image(image_np)\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    for prompt in text_prompts:\n",
    "        print(f\"Processing: {prompt}\")\n",
    "        boxes, scores = text_pipeline.text_to_bounding_box(\n",
    "            image_np, prompt, threshold=confidence_threshold\n",
    "        )\n",
    "\n",
    "        prompt_masks = []\n",
    "        for box in boxes:\n",
    "            input_box = np.array(box)\n",
    "            masks, scores, logits = predictor.predict(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                box=input_box[None, :],\n",
    "                multimask_output=True,\n",
    "            )\n",
    "            best_mask_idx = np.argmax(scores)\n",
    "            prompt_masks.append(masks[best_mask_idx])\n",
    "\n",
    "        all_results[prompt] = {\n",
    "            'masks': prompt_masks,\n",
    "            'boxes': boxes,\n",
    "            'count': len(prompt_masks)\n",
    "        }\n",
    "\n",
    "    return all_results\n",
    "\n",
    "def apply_masks_to_image_fixed(image, all_results):\n",
    "    \"\"\"Apply multiple masks to image with different colors - FIXED VERSION\"\"\"\n",
    "    image_np = image.copy() if isinstance(image, np.ndarray) else np.array(image)\n",
    "    result_image_multi = image_np.copy().astype(np.float32)\n",
    "\n",
    "    colors = {\n",
    "        \"person\": [255, 0, 0],    # Red\n",
    "        \"dog\": [0, 255, 0],      # Green\n",
    "        \"cat\": [0, 255, 255],    # Yellow\n",
    "        \"car\": [0, 0, 255],      # Blue\n",
    "        \"bird\": [255, 0, 255],   # Magenta\n",
    "        \"chair\": [255, 165, 0],  # Orange\n",
    "    }\n",
    "\n",
    "    for prompt, result in all_results.items():\n",
    "        color = np.array(colors.get(prompt, [255, 255, 0]))  # Default to yellow\n",
    "        print(f\"Applying {len(result['masks'])} mask(s) for '{prompt}' with color {color}\")\n",
    "\n",
    "        for mask in result['masks']:\n",
    "            # Ensure mask is boolean\n",
    "            if mask.dtype != bool:\n",
    "                boolean_mask = mask.astype(bool)\n",
    "            else:\n",
    "                boolean_mask = mask\n",
    "\n",
    "            print(f\"  Mask shape: {boolean_mask.shape}, True pixels: {np.sum(boolean_mask)}\")\n",
    "\n",
    "            # Apply color to masked regions\n",
    "            for c in range(3):\n",
    "                result_image_multi[boolean_mask, c] = (\n",
    "                    result_image_multi[boolean_mask, c] * 0.3 +\n",
    "                    color[c] * 0.7\n",
    "                )\n",
    "\n",
    "    # Convert back to uint8\n",
    "    result_image_multi = np.clip(result_image_multi, 0, 255).astype(np.uint8)\n",
    "    return result_image_multi\n",
    "\n",
    "# Example with multiple objects\n",
    "multi_prompts = [\"person\", \"dog\", \"car\"]  # @param\n",
    "multi_confidence = 0.1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "\n",
    "print(\"Performing multi-object segmentation...\")\n",
    "multi_results = multi_object_segmentation_fixed(image, multi_prompts, multi_confidence)\n",
    "\n",
    "# Apply masks with fixed function\n",
    "result_image_multi = apply_masks_to_image_fixed(image, multi_results)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(result_image_multi)\n",
    "plt.title(f'Multi-Object Segmentation: {\", \".join(multi_prompts)}')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print results summary\n",
    "for prompt, result in multi_results.items():\n",
    "    print(f\"Found {result['count']} {prompt}(s)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
